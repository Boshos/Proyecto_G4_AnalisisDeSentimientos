{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xsgb-e6NGuwI"
   },
   "source": [
    "##Notebook: 02_Modelado_Baseline_Sentiment.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "np3ba2HWHGnt"
   },
   "source": [
    "Montar Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "775pQ9_VGsFM",
    "outputId": "7f472be8-624e-4832-d1b9-f179ef5fb778"
   },
   "outputs": [],
   "source": [
    "#montar el drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLYCpHkXHKG0"
   },
   "source": [
    " Dependencias, seed y utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dXyeqdhIoCmg",
    "outputId": "651f45a0-b3c6-4ccd-cc0e-a75dd7dc6b79"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Agregar carpeta src al path\n",
    "sys.path.append(str(Path(\"/content/drive/MyDrive/Proyecto_Analisis_de_Sentimientos_G4\") / \"src\"))\n",
    "\n",
    "import importlib\n",
    "import utils.config_rutas as cr\n",
    "importlib.reload(cr)\n",
    "\n",
    "print(\"Usando:\", cr.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TbUaLbUBHLkS"
   },
   "outputs": [],
   "source": [
    "# instalar dependencias que se usa para el notebook\n",
    "!pip -q install scikit-learn pandas numpy matplotlib joblib\n",
    "\n",
    "#import y utilidades\n",
    "import os, json, time, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, ConfusionMatrixDisplay\n",
    "from utils import config_rutas as rutas\n",
    "import joblib\n",
    "\n",
    "# Semilla para resultados reproducibles (mismos valores)\n",
    "seed = 42\n",
    "random.seed(seed); np.random.seed(seed)\n",
    "\n",
    "# Identificador de archivos\n",
    "id_arch = \"02_\"\n",
    "\n",
    "#Utilidades de guardado\n",
    "def _ensure_parent(p):\n",
    "    p = Path(p); p.parent.mkdir(parents=True, exist_ok=True); return p\n",
    "\n",
    "def save_json(obj, path):\n",
    "    p = _ensure_parent(path)\n",
    "    with open(p, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "    print(\"JSON guardado en:\", p)\n",
    "\n",
    "def save_fig(path):\n",
    "    p = _ensure_parent(path)\n",
    "    plt.savefig(p, dpi=160, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"Imagen guardada en:\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHKMa27FHV8l"
   },
   "source": [
    "Cargar dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249
    },
    "id": "dZUnwdzyHUUt",
    "outputId": "e46d9214-c2cb-42ca-fb9b-c70b583c014e"
   },
   "outputs": [],
   "source": [
    "# Ruta del archivo limpio y balanceado\n",
    "raw_dir = rutas.get_raw_dir()\n",
    "\n",
    "# Ajusta el nombre si tu archivo se llama distinto\n",
    "data_csv = raw_dir / \"amazon_reviews_limpio_balanceado.csv\"\n",
    "\n",
    "# Carga del csv\n",
    "df = pd.read_csv(data_csv)\n",
    "print(\"Tamaño del dataset:\", df.shape)\n",
    "print(\"Columnas:\", df.columns.tolist())\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxG40vtbH2R6"
   },
   "source": [
    "Elegir columnas (texto y etiqueta) con auto-detección"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7QGelVjOH23y",
    "outputId": "aa61c542-7bcb-4acd-edef-dffb9078eda8"
   },
   "outputs": [],
   "source": [
    "# Realizar la deteccion automatica\n",
    "cands_text = [\n",
    "    \"reviewText\",\"review_text\",\"review_body\",\"text\",\"body\",\"content\",\"review\"\n",
    "]\n",
    "cands_label = [\n",
    "    \"sentiment\",\"label\",\"target\",\"y\",\n",
    "    \"overall\",\"rating\",\"stars\",\"star_rating\",\"score\"\n",
    "]\n",
    "\n",
    "text_col = next((c for c in cands_text if c in df.columns), None)\n",
    "label_col = next((c for c in cands_label if c in df.columns), None)\n",
    "\n",
    "#Mapeo de a puntuacion (1-5) a sentimientos ya que la columna es numerica\n",
    "def map_stars_to_sentiment(x):\n",
    "    \"\"\"Convierte 1-2 -> negative, 3 -> neutral, 4-5 -> positive.\"\"\"\n",
    "    try:\n",
    "        v = int(float(x))\n",
    "    except:\n",
    "        return None\n",
    "    if v in [1,2]: return \"negative\"\n",
    "    if v == 3:     return \"neutral\"\n",
    "    if v in [4,5]: return \"positive\"\n",
    "    return None\n",
    "\n",
    "# etiqueta es de estrellas puntuadas:\n",
    "if label_col in [\"overall\",\"rating\",\"stars\",\"star_rating\",\"score\"]:\n",
    "    df[\"label_mapped\"] = df[label_col].apply(map_stars_to_sentiment)\n",
    "    label_col = \"label_mapped\"\n",
    "\n",
    "print(\"text_col =\", text_col, \" | label_col =\", label_col)\n",
    "assert text_col is not None and label_col is not None, \"Ajusta text_col/label_col y vuelve a ejecutar.\"\n",
    "\n",
    "# Realizar una limpieza basica para evitar errores\n",
    "df = df[[text_col, label_col]].dropna()\n",
    "df = df[df[text_col].astype(str).str.strip()!=\"\"]\n",
    "# Normaliza a minúsculas las etiquetas\n",
    "df[label_col] = df[label_col].astype(str).str.lower().str.strip()\n",
    "\n",
    "# Guarda una copia limpia de referencia\n",
    "# Ruta donde guardar el csv limpio\n",
    "csv_limpio = rutas.get_processed_dir() / f\"{id_arch}clean.csv\"\n",
    "\n",
    "# Guardar el archivo\n",
    "df.to_csv(csv_limpio, index=False, encoding=\"utf-8\")\n",
    "print(\"Dataset limpio guardado en:\", csv_limpio)\n",
    "\n",
    "# Ver balance de clases\n",
    "print(df[label_col].value_counts())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOJg4JfxH675"
   },
   "source": [
    "Split y pipeline (TF-IDF + Regresión Logística)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yTYvB0I9KfAu",
    "outputId": "19ee060b-dc17-4bef-db1b-8dea63765db9"
   },
   "outputs": [],
   "source": [
    "# Separar en treintesto 80%-20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[text_col], df[label_col], test_size=0.2, stratify=df[label_col], random_state=seed\n",
    ")\n",
    "\n",
    "#PIPELINE: TF-IDF (unigramas+bigramas) + Regresión Logística\n",
    "pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        ngram_range=(1,2),\n",
    "        min_df=2,\n",
    "        max_features=50000\n",
    "    )),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        class_weight=\"balanced\",  # por si hay leve desbalance\n",
    "        solver=\"lbfgs\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Entrenar\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar\n",
    "preds = pipeline.predict(X_test)\n",
    "acc = accuracy_score(y_test, preds)\n",
    "f1m = f1_score(y_test, preds, average=\"macro\")\n",
    "print(f\"Accuracy: {acc:.4f} | F1-macro: {f1m:.4f}\")\n",
    "print(classification_report(y_test, preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yvk5fuPJaAU"
   },
   "source": [
    "Matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "id": "5yLdcpPJJZpg",
    "outputId": "d150e3b1-07e1-4771-ba18-5e9166730408"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "labels = sorted(df[label_col].unique())\n",
    "cm = confusion_matrix(y_test, preds, labels=labels)\n",
    "\n",
    "plt.figure(figsize=(4.8,4.2))\n",
    "plt.imshow(cm, interpolation='nearest')\n",
    "plt.title('Confusion Matrix (baseline)')\n",
    "plt.xticks(range(len(labels)), labels, rotation=45)\n",
    "plt.yticks(range(len(labels)), labels)\n",
    "th = cm.max() / 2.0\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, cm[i, j], ha=\"center\",\n",
    "             color=\"white\" if cm[i, j] > th else \"black\")\n",
    "plt.tight_layout()\n",
    "plt.xlabel('Predicted'); plt.ylabel('True')\n",
    "plt.show()\n",
    "# Ruta para guardar la matriz de confusión\n",
    "cm_path = rutas.get_images_dir() / f\"{id_arch}confusion_matrix.png\"\n",
    "\n",
    "# Guardar la figura\n",
    "save_fig(cm_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEuQZo_VH9cj"
   },
   "source": [
    "Guardar artefactos: modelo, config, evaluación y predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h-WPnEylH_BK",
    "outputId": "28803332-0a54-4bb3-bcf3-412965555f44"
   },
   "outputs": [],
   "source": [
    "#Rutas de salida en donde se guardaran los artefactos\n",
    "model_path = rutas.get_models_dir() / f\"{id_arch}sentiment_logreg_tfidf.joblib\"\n",
    "cfg_path   = rutas.configs_dir / f\"{id_arch}sentiment_logreg_tfidf.json\"\n",
    "eval_path  = rutas.get_eval_dir() / f\"{id_arch}sentiment_eval.json\"\n",
    "preds_csv  = rutas.get_processed_dir() / f\"{id_arch}preds_sentiment.csv\"\n",
    "\n",
    "# Guardar modelo\n",
    "joblib.dump(pipeline, model_path)\n",
    "print(\"Modelo guardado en:\", model_path)\n",
    "\n",
    "# Guardar configuración\n",
    "config = {\n",
    "    \"model\": \"logreg_tfidf\",\n",
    "    \"tfidf\": {\"ngram_range\": [1,2], \"min_df\": 2, \"max_features\": 50000},\n",
    "    \"split\": {\"test_size\": 0.2, \"stratify\": True, \"seed\": seed},\n",
    "    \"labels\": sorted(df[label_col].unique())\n",
    "}\n",
    "save_json(config, cfg_path)\n",
    "print(\"Config guardada en:\", cfg_path)\n",
    "\n",
    "# Guardar las métricas\n",
    "evaluation = {\n",
    "    \"accuracy\": float(acc),\n",
    "    \"f1_macro\": float(f1m),\n",
    "    \"report\": classification_report(y_test, preds, output_dict=True)\n",
    "}\n",
    "save_json(evaluation, eval_path)\n",
    "print(\"Evaluación guardada en:\", eval_path)\n",
    "\n",
    "# Guardar predicciones\n",
    "pd.DataFrame({\n",
    "    \"text\": X_test.values,\n",
    "    \"y_true\": y_test.values,\n",
    "    \"y_pred\": preds\n",
    "}).to_csv(preds_csv, index=False, encoding=\"utf-8\")\n",
    "print(\"Predicciones guardadas en:\", preds_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngos18nHzcv5"
   },
   "source": [
    "GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DoEbWlaZyVoS",
    "outputId": "21f22780-938f-433a-fcf4-84ab74ceebb4"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Espacio de búsqueda\n",
    "param_grid = {\n",
    "    \"tfidf__ngram_range\": [(1,1), (1,2)],\n",
    "    \"tfidf__min_df\": [1, 2, 5],\n",
    "    \"tfidf__max_features\": [20000, 50000, None],\n",
    "    \"clf__C\": [0.1, 0.5, 1.0, 2.0, 10.0],\n",
    "    \"clf__class_weight\": [None, \"balanced\"],\n",
    "    \"clf__penalty\": [\"l2\"],\n",
    "    \"clf__solver\": [\"lbfgs\"],\n",
    "    \"clf__max_iter\": [1000, 2000],\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "gs = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"f1_macro\",            # métrica objetivo\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    refit=True,                    # reentrena el mejor con todo el train\n",
    "    verbose=2,\n",
    "    return_train_score=False\n",
    ")\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Best params:\", gs.best_params_)\n",
    "print(\"Best CV F1_macro:\", gs.best_score_)\n",
    "\n",
    "# Evaluación en test con el mejor modelo\n",
    "best_model = gs.best_estimator_\n",
    "preds = best_model.predict(X_test)\n",
    "acc  = accuracy_score(y_test, preds)\n",
    "f1m  = f1_score(y_test, preds, average=\"macro\")\n",
    "print(f\"Test Accuracy: {acc:.4f} | Test F1_macro: {f1m:.4f}\")\n",
    "print(classification_report(y_test, preds))\n",
    "\n",
    "# Guardar artefactos\n",
    "MODELS  = rutas.get_models_dir()\n",
    "CONFIGS = rutas.configs_dir         # atributo del módulo\n",
    "EVAL    = rutas.get_eval_dir()\n",
    "\n",
    "model_path     = MODELS  / f\"{id_arch}baseline_best.joblib\"\n",
    "params_path    = CONFIGS / f\"{id_arch}baseline_gridsearch_params.json\"\n",
    "cvresults_path = EVAL    / f\"{id_arch}baseline_gridsearch_results.csv\"\n",
    "test_eval_path = EVAL    / f\"{id_arch}baseline_test_eval.json\"\n",
    "\n",
    "# a) Mejor modelo\n",
    "joblib.dump(best_model, model_path)\n",
    "# b) Parámetros óptimos\n",
    "with open(params_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(gs.best_params_, f, ensure_ascii=False, indent=2)\n",
    "# c) Resultados completos del grid\n",
    "pd.DataFrame(gs.cv_results_).to_csv(cvresults_path, index=False, encoding=\"utf-8\")\n",
    "# d) Métricas en test\n",
    "with open(test_eval_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\"accuracy\": float(acc), \"f1_macro\": float(f1m),\n",
    "         \"report\": classification_report(y_test, preds, output_dict=True)},\n",
    "        f, ensure_ascii=False, indent=2\n",
    "    )\n",
    "\n",
    "print(\"Guardado:\")\n",
    "print(\"  Modelo ->\", model_path)\n",
    "print(\"  Best params ->\", params_path)\n",
    "print(\"  CV results ->\", cvresults_path)\n",
    "print(\"  Test eval ->\", test_eval_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "id": "GqxDrHW8PuI6",
    "outputId": "0cc7cfbe-7b84-4c53-8d85-d9317afc6a2c"
   },
   "outputs": [],
   "source": [
    "# Predicciones con probabilidades + Matriz de confusión (best_model)\n",
    "\n",
    "EVAL = rutas.get_eval_dir()\n",
    "IMGS = rutas.get_images_dir()\n",
    "\n",
    "# Helper para guardar figuras con creación de carpeta\n",
    "def _save_fig(path):\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(path, dpi=160, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"Figura guardada en:\", path)\n",
    "\n",
    "# Predicciones y probabilidades\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "proba = None\n",
    "if hasattr(best_model, \"predict_proba\"):\n",
    "    proba = best_model.predict_proba(X_test)\n",
    "elif hasattr(best_model, \"named_steps\") and hasattr(best_model.named_steps.get(\"clf\", None), \"predict_proba\"):\n",
    "    proba = best_model.named_steps[\"clf\"].predict_proba(best_model.named_steps[\"tfidf\"].transform(X_test))\n",
    "\n",
    "# Construir DataFrame de salida\n",
    "preds_df = pd.DataFrame({\n",
    "    \"text\": np.array(X_test),\n",
    "    \"y_true\": np.array(y_test),\n",
    "    \"y_pred\": y_pred\n",
    "})\n",
    "\n",
    "if proba is not None:\n",
    "    # clases desde el clasificador del pipeline\n",
    "    classes = best_model.named_steps[\"clf\"].classes_.tolist()\n",
    "    for i, c in enumerate(classes):\n",
    "        preds_df[f\"proba_{c}\"] = proba[:, i]\n",
    "\n",
    "# Guardar CSV con predicciones\n",
    "preds_csv = EVAL / f\"{id_arch}preds_with_proba.csv\"\n",
    "preds_df.to_csv(preds_csv, index=False, encoding=\"utf-8\")\n",
    "print(\"Predicciones guardadas en:\", preds_csv)\n",
    "\n",
    "# Matriz de confusión\n",
    "labels = sorted(pd.Series(y_test).unique().tolist())\n",
    "cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot()\n",
    "plt.title(\"Confusion Matrix - Best Baseline\")\n",
    "plt.show()\n",
    "cm_png = IMGS / f\"{id_arch}confusion_matrix.png\"\n",
    "save_fig(cm_png)\n",
    "\n",
    "\n",
    "# Guardar classification_report a JSON\n",
    "cr_json = EVAL / f\"{id_arch}classification_report.json\"\n",
    "with open(cr_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(classification_report(y_test, y_pred, output_dict=True),\n",
    "              f, ensure_ascii=False, indent=2)\n",
    "print(\"Classification report guardado en:\", cr_json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFO4IhZ5J5rU"
   },
   "source": [
    " ABSA (Aspect-Based) baseline por diccionario + gráfico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "id": "xHrADjwFJ5GT",
    "outputId": "8b7ee669-fea1-47ed-df00-4b1f1b7043f2"
   },
   "outputs": [],
   "source": [
    "#unidecode sirve para quitar acentos y caracteres especiales y convertirlos a caracteres ASCII simples.\n",
    "\n",
    "import re\n",
    "try:\n",
    "    from unidecode import unidecode  # pip install Unidecode\n",
    "    # fallback simple si no está instalado\n",
    "    def unidecode(x):\n",
    "        return x\n",
    "\n",
    "# Palabras clave por aspecto (ES + EN)\n",
    "# palabras ocupadas en las reseñas teniendo en cuenta que la base es de reviews de celulares y sus accesorios\n",
    "words_reviews = {\n",
    "    \"price\": [\n",
    "        \"precio\",\"costoso\",\"caro\",\"barato\",\"oferta\",\"descuento\",\n",
    "        \"price\",\"cost\",\"expensive\",\"cheap\",\"deal\",\"offer\"\n",
    "    ],\n",
    "    \"battery\": [\n",
    "        \"bateria\",\"carga\",\"cargando\",\"duracion\",\"se descarga\",\n",
    "        \"battery\",\"charge\",\"charging\",\"battery life\",\"lasts\",\"drain\"\n",
    "    ],\n",
    "    \"screen\": [\n",
    "        \"pantalla\",\"brillo\",\"resolucion\",\"oled\",\"amoled\",\"lcd\",\n",
    "        \"screen\",\"display\",\"brightness\",\"resolution\",\"oled\",\"amoled\",\"lcd\"\n",
    "    ],\n",
    "    \"camera\": [\n",
    "        \"camara\",\"foto\",\"fotos\",\"imagen\",\"video\",\"estabilizacion\",\n",
    "        \"camera\",\"photo\",\"picture\",\"video\",\"stabilization\",\"stabilisation\"\n",
    "    ],\n",
    "    \"performance\": [\n",
    "        \"rendimiento\",\"lento\",\"rapido\",\"fluido\",\"lag\",\"procesador\",\"chip\",\"ram\",\n",
    "        \"performance\",\"slow\",\"fast\",\"snappy\",\"lag\",\"processor\",\"chip\",\"ram\"\n",
    "    ],\n",
    "    \"audio\": [\n",
    "        \"sonido\",\"audio\",\"volumen\",\"parlante\",\"bocina\",\"auricular\",\"microfono\",\"mic\",\n",
    "        \"sound\",\"audio\",\"volume\",\"speaker\",\"earphone\",\"headphone\",\"microphone\",\"mic\"\n",
    "    ],\n",
    "    \"connectivity\": [\n",
    "        \"senal\",\"wifi\",\"bluetooth\",\"5g\",\"lte\",\"datos\",\"red\",\n",
    "        \"signal\",\"wifi\",\"bluetooth\",\"5g\",\"lte\",\"data\",\"network\"\n",
    "    ],\n",
    "    \"build_quality\": [\n",
    "        \"calidad\",\"material\",\"defecto\",\"defectuoso\",\"fragil\",\"resistente\",\"durable\",\n",
    "        \"quality\",\"material\",\"defect\",\"defective\",\"fragile\",\"sturdy\",\"durable\"\n",
    "    ],\n",
    "    \"shipping\": [\n",
    "        \"envio\",\"entrega\",\"llego\",\"tarde\",\"paquete\",\"empaque\",\"courier\",\n",
    "        \"shipping\",\"delivery\",\"arrived\",\"late\",\"package\",\"packaging\",\"courier\"\n",
    "    ],\n",
    "    \"accessories\": [\n",
    "        \"cargador\",\"funda\",\"cable\",\"protector\",\"auriculares\",\"audifonos\",\"earbuds\",\n",
    "        \"charger\",\"case\",\"cable\",\"protector\",\"earbuds\",\"headphones\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "def detect_aspects_rule(text: str):\n",
    "    \"\"\"Detecta aspectos por reglas ES+EN con coincidencia por palabra completa.\"\"\"\n",
    "    t = unidecode(str(text).lower())\n",
    "    found = set()\n",
    "    for aspect, kws in words_reviews.items():\n",
    "        for kw in kws:\n",
    "            pat = rf\"\\b{re.escape(unidecode(kw.lower()))}\\b\"\n",
    "            if re.search(pat, t):\n",
    "                found.add(aspect)\n",
    "                break\n",
    "    return list(found)\n",
    "\n",
    "# Construir DF ABSA para el test\n",
    "absa = pd.DataFrame({\n",
    "    \"text\": X_test.values,\n",
    "    \"sentiment_pred\": preds,\n",
    "    \"aspects\": [detect_aspects_rule(t) for t in X_test.values]\n",
    "})\n",
    "\n",
    "# Rutas de salida (helpers)\n",
    "PROC = rutas.get_processed_dir()   # data/processed\n",
    "IMGS = rutas.get_images_dir()      # docs/images\n",
    "\n",
    "# Guardar CSV ABSA\n",
    "absa_csv = PROC / f\"{id_arch}absa_baseline.csv\"\n",
    "absa.to_csv(absa_csv, index=False, encoding=\"utf-8\")\n",
    "print(\"ABSA baseline guardado en:\", absa_csv)\n",
    "\n",
    "# TOP-10 de aspectos y gráfico\n",
    "aspect_counts = (\n",
    "    absa.explode(\"aspects\")\n",
    "        .dropna()\n",
    "        .groupby(\"aspects\")\n",
    "        .size()\n",
    "        .sort_values(ascending=False)\n",
    ")\n",
    "top = aspect_counts.head(10)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "top.plot(kind=\"bar\")\n",
    "plt.title(\"Top-10 detected aspects (baseline)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "absa_fig = IMGS / f\"{id_arch}absa_counts.png\"\n",
    "save_fig(absa_fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAs3l_UGKslw"
   },
   "source": [
    "Urgencia (baseline EN) + export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "peS69S2uKujZ",
    "outputId": "9a3b7e2f-5bb2-408a-9f96-009706ba87b2"
   },
   "outputs": [],
   "source": [
    "from utils import config_rutas as cr  # Importa las rutas del proyecto\n",
    "\n",
    "# Lista de palabras clave que indican \"urgencia\" en reseñas (en inglés)\n",
    "# Puedes adaptarla según tu dominio o idioma\n",
    "urgency_keys = [\n",
    "    \"complaint\", \"return\", \"refund\", \"doesn't work\", \"does not work\",\n",
    "    \"defective\", \"warranty\", \"scam\", \"broken\", \"won't charge\", \"does not charge\",\n",
    "    \"fault\", \"bad\", \"terrible\", \"cancel\", \"missing\", \"lost\", \"damage\", \"damaged\"\n",
    "]\n",
    "\n",
    "def urgency_score(text: str):\n",
    "    \"\"\"\n",
    "    Calcula un puntaje de urgencia en el rango [0, 1] para un texto.\n",
    "    \"\"\"\n",
    "#  - Convierte a minúsculas para búsqueda insensible a mayúsculas.\n",
    "#  - Suma 0.2 por cada palabra clave encontrada.\n",
    "    t = str(text).lower()  # Texto en minúsculas\n",
    "    base = 0.2             # Puntaje inicial\n",
    "    hits = sum(1 for k in urgency_keys if k in t)  # Cantidad de coincidencias\n",
    "    return float(min(1.0, base + 0.2 * hits))      # Puntaje final limitado a 1.0\n",
    "\n",
    "\n",
    "urg = pd.DataFrame({\n",
    "    \"text\": X_test.values,\n",
    "    \"sentiment_pred\": preds,\n",
    "    \"urgency\": [urgency_score(t) for t in X_test.values]\n",
    "})\n",
    "\n",
    "# Ruta para guardar el CSV de urgencia baseline en /data/processed/\n",
    "urg_csv = cr.get_processed_dir() / f\"{id_arch}urgency_baseline.csv\"\n",
    "urg.to_csv(urg_csv, index=False, encoding=\"utf-8\")\n",
    "print(\"Urgency baseline guardado en:\", urg_csv)\n",
    "\n",
    "# Ordenar por urgencia y quedarse con los 10 casos más urgentes\n",
    "urgent_top = urg.sort_values(\"urgency\", ascending=False).head(10)\n",
    "\n",
    "# Ruta para guardar el CSV del Top-10 en /notebooks/evaluation/\n",
    "urgent_top10_csv = cr.get_eval_dir() / f\"{id_arch}urgent_top.csv\"\n",
    "urgent_top.to_csv(urgent_top10_csv, index=False, encoding=\"utf-8\")\n",
    "print(\"Top-10 urgencias guardado en:\", urgent_top10_csv)\n",
    "\n",
    "# Mostrar en pantalla el Top-10\n",
    "urgent_top\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLLrG9hzK0SG"
   },
   "source": [
    "Cargar el modelo guardado y hacer inferencia rápida (EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E476JQakKyd9",
    "outputId": "18e21227-fa14-4314-aa93-dd33f43baf65"
   },
   "outputs": [],
   "source": [
    "#Rutas de salida en donde se guardaran los artefactos\n",
    "MODELS = rutas.get_models_dir()\n",
    "EVAL   = rutas.get_eval_dir()\n",
    "\n",
    "# Se carga el pipeline (TF-IDF + Regresión Logística) guardado previamente\n",
    "model_path = MODELS / f\"{id_arch}sentiment_logreg_tfidf.joblib\"\n",
    "loaded = joblib.load(model_path)\n",
    "print(\"Modelo cargado desde:\", model_path)\n",
    "\n",
    "def predict_texts(texts):\n",
    "    \"\"\"\n",
    "    Devuelve la etiqueta de sentimiento ('positive'/'neutral'/'negative')\n",
    "    para cada texto usando el modelo cargado.\n",
    "    \"\"\"\n",
    "    return loaded.predict(texts)\n",
    "\n",
    "# Prueba de inferencia\n",
    "examples = [\n",
    "    \"Great camera and battery life, totally recommended.\",\n",
    "    \"Arrived late and the charger doesn't work. Terrible.\",\n",
    "    \"Fair price, decent screen, nothing extraordinary.\"\n",
    "]\n",
    "\n",
    "preds_demo = list(zip(examples, predict_texts(examples)))\n",
    "for text, pred in preds_demo:\n",
    "    print(f\"[{pred.upper()}] {text}\")\n",
    "\n",
    "#  Guardar resultados de la prueba\n",
    "demo_csv = EVAL / \"sample_inference.csv\"\n",
    "pd.DataFrame(preds_demo, columns=[\"text\", \"prediction\"]).to_csv(\n",
    "    demo_csv, index=False, encoding=\"utf-8\"\n",
    ")\n",
    "print(\"Inferencias de ejemplo guardadas en:\", demo_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIrWPtdRvcRe"
   },
   "source": [
    "Simulacion tiempo real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cPRYpmenuh7G",
    "outputId": "edaaaf02-8003-460e-fd87-fab7672418e3"
   },
   "outputs": [],
   "source": [
    "#Rutas de salida\n",
    "EVAL   = rutas.get_eval_dir()\n",
    "MODELS = rutas.get_models_dir()\n",
    "out_csv = EVAL / f\"{id_arch}realtime_events.csv\"\n",
    "\n",
    "#Asegurar modelo listo\n",
    "best_model = globals().get(\"best_model\")\n",
    "if best_model is None:\n",
    "    # intenta cargar desde disco\n",
    "    candidates = [\n",
    "        MODELS / f\"{id_arch}baseline_best.joblib\",\n",
    "        MODELS / f\"{id_arch}sentiment_logreg_tfidf.joblib\",\n",
    "    ]\n",
    "    loaded = None\n",
    "    for p in candidates:\n",
    "        if Path(p).exists():\n",
    "            loaded = joblib.load(p)\n",
    "            print(\"Modelo cargado desde:\", p)\n",
    "            break\n",
    "    if loaded is None:\n",
    "        # fallback: si tienes 'pipeline' en memoria\n",
    "        loaded = globals().get(\"pipeline\")\n",
    "        if loaded is not None:\n",
    "            print(\"Usando 'pipeline' en memoria como modelo.\")\n",
    "        else:\n",
    "            raise RuntimeError(\"No encontré modelo. Entrena o guarda el baseline antes de correr tiempo real.\")\n",
    "    best_model = loaded\n",
    "\n",
    "#Fallbacks\n",
    "def _fallback_urgency_score(text: str) -> float:\n",
    "    keys = [\"complaint\",\"return\",\"refund\",\"broken\",\"defective\",\"warranty\",\n",
    "            \"late\",\"missing\",\"lost\",\"damage\",\"damaged\",\"won't charge\",\"does not work\"]\n",
    "    t = str(text).lower()\n",
    "    base = 0.2\n",
    "    hits = sum(1 for k in keys if k in t)\n",
    "    return float(min(1.0, base + 0.2 * hits))\n",
    "\n",
    "def _fallback_reply_es(sentiment, aspects, u):\n",
    "    def lvl(x): return \"alta\" if x >= 0.8 else \"media\" if x >= 0.5 else \"baja\"\n",
    "    ax = \", \".join(aspects) if aspects else \"el producto\"\n",
    "    ustr = lvl(u)\n",
    "    if sentiment == \"negative\":\n",
    "        if ustr == \"alta\":\n",
    "            return (f\"Lamentamos lo ocurrido con {ax}. Escalamos tu caso para atención prioritaria. \"\n",
    "                    \"Por favor envía tu número de pedido para resolverlo cuanto antes.\")\n",
    "        if ustr == \"media\":\n",
    "            return (f\"Gracias por avisarnos sobre {ax}. ¿Puedes compartir más detalles o una foto del problema?\")\n",
    "        return f\"Lamentamos tu experiencia con {ax}. Tomamos nota para mejorar.\"\n",
    "    if sentiment == \"positive\":\n",
    "        return f\"¡Gracias! Nos alegra tu buena experiencia con {ax}.\"\n",
    "    return f\"Gracias por tu comentario sobre {ax}. Si necesitas ayuda, estamos atentos.\"\n",
    "\n",
    "\n",
    "urgency_score_fn   = globals().get(\"urgency_score\", _fallback_urgency_score)\n",
    "generate_reply_fn  = globals().get(\"generate_reply_es\", _fallback_reply_es)\n",
    "\n",
    "#Función que procesa un evento\n",
    "def process_event(text: str) -> dict:\n",
    "    s = best_model.predict([text])[0]        # sentimiento\n",
    "    aspects = detect_aspects_rule(text)      # aspectos (tu regla)\n",
    "    u = float(urgency_score_fn(text))        # urgencia [0..1]\n",
    "    reply = generate_reply_fn(s, aspects, u) if generate_reply_fn is _fallback_reply_es else generate_reply_fn(text, s, aspects, u)\n",
    "    return {\"text\": text, \"sentiment\": s, \"aspects\": aspects, \"urgency_score\": u, \"reply\": reply}\n",
    "\n",
    "#  Simulador de streaming\n",
    "def run_stream(text_iterable, sleep_s=0.2, save_every=5):\n",
    "    \"\"\"\n",
    "    Procesa textos uno a uno e imprime resultados; guarda acumulado en CSV.\n",
    "    - text_iterable: iterable de strings\n",
    "    - sleep_s: retardo entre eventos para simular llegada gradual\n",
    "    - save_every: cada N eventos, escribe CSV\n",
    "    \"\"\"\n",
    "    events = []\n",
    "    count = 0\n",
    "    for t in text_iterable:\n",
    "        out = process_event(t)\n",
    "        events.append(out)\n",
    "        count += 1\n",
    "        print(f\"[{out['sentiment'].upper()} | urg={out['urgency_score']:.2f}] {t}\")\n",
    "        print(\"→\", out[\"reply\"])\n",
    "        print(\"-\"*60)\n",
    "        if count % save_every == 0:\n",
    "            pd.DataFrame(events).to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "        time.sleep(sleep_s)\n",
    "    pd.DataFrame(events).to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "    print(\"Eventos exportados a:\", out_csv)\n",
    "\n",
    "# ejemplo\n",
    "queue_texts = [\n",
    "    \"La batería se descarga muy rápido y el cargador no funciona.\",\n",
    "    \"Llegó a tiempo y la cámara es excelente.\",\n",
    "    \"La pantalla está bien, pero el precio me pareció alto.\",\n",
    "    \"Refund requested, item arrived damaged and the box was open.\",\n",
    "]\n",
    "run_stream(queue_texts, sleep_s=0.15, save_every=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fuBgTAJWVHR"
   },
   "source": [
    "#Conclusiones – Notebook 02_Modelo_Baseline\n",
    "\n",
    "* **Implementación y objetivo del modelo**  \n",
    "   - Se construyó un **modelo base (baseline)** para establecer un punto de referencia inicial en la tarea de análisis de sentimientos.  \n",
    "\n",
    "* **Resultados clave**  \n",
    "   - El modelo alcanzó una precisión global (accuracy) del 75% y un F1-macro de 0.7516, lo que indica un rendimiento equilibrado en las tres clases de sentimiento (positivo, negativo y neutral).\n",
    "   - Negativo: Precisión 0.79, Recall 0.77, F1-score 0.78\n",
    "   -Neutral: Precisión 0.66, Recall 0.68, F1-score 0.67\n",
    "   -Positivo: Precisión 0.81, Recall 0.79, F1-score 0.80\n",
    "* La integración del procesamiento en tiempo  real posibilita analizar y clasificar reseñas inmediatamente después de su recepción, mejorando la capacidad de respuesta de la organización ante problemas críticos. Esto, combinado con la clasificación de urgencia, facilita la priorización de casos que requieren atención inmediata."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
