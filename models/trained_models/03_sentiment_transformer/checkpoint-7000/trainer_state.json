{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.3333333333333335,
  "eval_steps": 500,
  "global_step": 7000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.016666666666666666,
      "grad_norm": 3.115821599960327,
      "learning_rate": 2.9836666666666665e-05,
      "loss": 1.0706,
      "step": 50
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 1.608338713645935,
      "learning_rate": 2.967e-05,
      "loss": 1.0693,
      "step": 100
    },
    {
      "epoch": 0.05,
      "grad_norm": 1.9303350448608398,
      "learning_rate": 2.9503333333333336e-05,
      "loss": 1.0678,
      "step": 150
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 4.235202312469482,
      "learning_rate": 2.933666666666667e-05,
      "loss": 1.0768,
      "step": 200
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 3.545532703399658,
      "learning_rate": 2.9170000000000004e-05,
      "loss": 1.0737,
      "step": 250
    },
    {
      "epoch": 0.1,
      "grad_norm": 2.185041666030884,
      "learning_rate": 2.9003333333333334e-05,
      "loss": 1.075,
      "step": 300
    },
    {
      "epoch": 0.11666666666666667,
      "grad_norm": 3.335514783859253,
      "learning_rate": 2.8836666666666668e-05,
      "loss": 1.0767,
      "step": 350
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 3.373791217803955,
      "learning_rate": 2.867e-05,
      "loss": 1.0696,
      "step": 400
    },
    {
      "epoch": 0.15,
      "grad_norm": 1.8721890449523926,
      "learning_rate": 2.8503333333333335e-05,
      "loss": 1.0818,
      "step": 450
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 1.672345519065857,
      "learning_rate": 2.833666666666667e-05,
      "loss": 1.0768,
      "step": 500
    },
    {
      "epoch": 0.18333333333333332,
      "grad_norm": 4.299988269805908,
      "learning_rate": 2.817e-05,
      "loss": 1.075,
      "step": 550
    },
    {
      "epoch": 0.2,
      "grad_norm": 1.6576850414276123,
      "learning_rate": 2.8003333333333333e-05,
      "loss": 1.088,
      "step": 600
    },
    {
      "epoch": 0.21666666666666667,
      "grad_norm": 3.133028268814087,
      "learning_rate": 2.7836666666666667e-05,
      "loss": 1.0793,
      "step": 650
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 1.9704374074935913,
      "learning_rate": 2.767e-05,
      "loss": 1.0857,
      "step": 700
    },
    {
      "epoch": 0.25,
      "grad_norm": 1.3103128671646118,
      "learning_rate": 2.7503333333333335e-05,
      "loss": 1.0677,
      "step": 750
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 2.716714859008789,
      "learning_rate": 2.733666666666667e-05,
      "loss": 1.0678,
      "step": 800
    },
    {
      "epoch": 0.2833333333333333,
      "grad_norm": 2.3759963512420654,
      "learning_rate": 2.717e-05,
      "loss": 1.071,
      "step": 850
    },
    {
      "epoch": 0.3,
      "grad_norm": 2.5315606594085693,
      "learning_rate": 2.7003333333333333e-05,
      "loss": 1.0699,
      "step": 900
    },
    {
      "epoch": 0.31666666666666665,
      "grad_norm": 1.4586771726608276,
      "learning_rate": 2.6836666666666667e-05,
      "loss": 1.0697,
      "step": 950
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 3.5174386501312256,
      "learning_rate": 2.667e-05,
      "loss": 1.0635,
      "step": 1000
    },
    {
      "epoch": 0.35,
      "grad_norm": 3.3450117111206055,
      "learning_rate": 2.6503333333333334e-05,
      "loss": 1.0735,
      "step": 1050
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 2.623202323913574,
      "learning_rate": 2.6336666666666668e-05,
      "loss": 1.0752,
      "step": 1100
    },
    {
      "epoch": 0.38333333333333336,
      "grad_norm": 2.063809394836426,
      "learning_rate": 2.617e-05,
      "loss": 1.0628,
      "step": 1150
    },
    {
      "epoch": 0.4,
      "grad_norm": 2.4334020614624023,
      "learning_rate": 2.6003333333333332e-05,
      "loss": 1.0714,
      "step": 1200
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 2.5566399097442627,
      "learning_rate": 2.5836666666666666e-05,
      "loss": 1.0757,
      "step": 1250
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 2.663787364959717,
      "learning_rate": 2.567e-05,
      "loss": 1.0792,
      "step": 1300
    },
    {
      "epoch": 0.45,
      "grad_norm": 3.7536184787750244,
      "learning_rate": 2.5503333333333334e-05,
      "loss": 1.0709,
      "step": 1350
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 2.4514963626861572,
      "learning_rate": 2.5336666666666664e-05,
      "loss": 1.0721,
      "step": 1400
    },
    {
      "epoch": 0.48333333333333334,
      "grad_norm": 1.599787950515747,
      "learning_rate": 2.517e-05,
      "loss": 1.0682,
      "step": 1450
    },
    {
      "epoch": 0.5,
      "grad_norm": 1.7431048154830933,
      "learning_rate": 2.5003333333333335e-05,
      "loss": 1.0795,
      "step": 1500
    },
    {
      "epoch": 0.5166666666666667,
      "grad_norm": 2.0197300910949707,
      "learning_rate": 2.483666666666667e-05,
      "loss": 1.0757,
      "step": 1550
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 1.4598742723464966,
      "learning_rate": 2.4670000000000003e-05,
      "loss": 1.0676,
      "step": 1600
    },
    {
      "epoch": 0.55,
      "grad_norm": 1.9365025758743286,
      "learning_rate": 2.4503333333333336e-05,
      "loss": 1.0643,
      "step": 1650
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 2.8979198932647705,
      "learning_rate": 2.4336666666666667e-05,
      "loss": 1.0739,
      "step": 1700
    },
    {
      "epoch": 0.5833333333333334,
      "grad_norm": 2.1364498138427734,
      "learning_rate": 2.417e-05,
      "loss": 1.0524,
      "step": 1750
    },
    {
      "epoch": 0.6,
      "grad_norm": 1.9569013118743896,
      "learning_rate": 2.4003333333333334e-05,
      "loss": 1.0751,
      "step": 1800
    },
    {
      "epoch": 0.6166666666666667,
      "grad_norm": 2.5429177284240723,
      "learning_rate": 2.3836666666666668e-05,
      "loss": 1.065,
      "step": 1850
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 2.740353584289551,
      "learning_rate": 2.3670000000000002e-05,
      "loss": 1.0707,
      "step": 1900
    },
    {
      "epoch": 0.65,
      "grad_norm": 1.6796435117721558,
      "learning_rate": 2.3503333333333336e-05,
      "loss": 1.0717,
      "step": 1950
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 2.0931904315948486,
      "learning_rate": 2.3336666666666666e-05,
      "loss": 1.0658,
      "step": 2000
    },
    {
      "epoch": 0.6833333333333333,
      "grad_norm": 2.539318561553955,
      "learning_rate": 2.317e-05,
      "loss": 1.0643,
      "step": 2050
    },
    {
      "epoch": 0.7,
      "grad_norm": 3.592773675918579,
      "learning_rate": 2.3003333333333334e-05,
      "loss": 1.0713,
      "step": 2100
    },
    {
      "epoch": 0.7166666666666667,
      "grad_norm": 3.856304883956909,
      "learning_rate": 2.2836666666666668e-05,
      "loss": 1.0647,
      "step": 2150
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 2.4438765048980713,
      "learning_rate": 2.267e-05,
      "loss": 1.069,
      "step": 2200
    },
    {
      "epoch": 0.75,
      "grad_norm": 3.4549388885498047,
      "learning_rate": 2.2503333333333332e-05,
      "loss": 1.0782,
      "step": 2250
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 2.3395016193389893,
      "learning_rate": 2.2336666666666666e-05,
      "loss": 1.0641,
      "step": 2300
    },
    {
      "epoch": 0.7833333333333333,
      "grad_norm": 1.4156253337860107,
      "learning_rate": 2.217e-05,
      "loss": 1.0718,
      "step": 2350
    },
    {
      "epoch": 0.8,
      "grad_norm": 2.500247001647949,
      "learning_rate": 2.2003333333333333e-05,
      "loss": 1.0722,
      "step": 2400
    },
    {
      "epoch": 0.8166666666666667,
      "grad_norm": 1.9317195415496826,
      "learning_rate": 2.1836666666666667e-05,
      "loss": 1.068,
      "step": 2450
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 1.6220333576202393,
      "learning_rate": 2.167e-05,
      "loss": 1.0694,
      "step": 2500
    },
    {
      "epoch": 0.85,
      "grad_norm": 1.7357357740402222,
      "learning_rate": 2.150333333333333e-05,
      "loss": 1.0549,
      "step": 2550
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 2.7378692626953125,
      "learning_rate": 2.1336666666666665e-05,
      "loss": 1.0762,
      "step": 2600
    },
    {
      "epoch": 0.8833333333333333,
      "grad_norm": 2.513519525527954,
      "learning_rate": 2.117e-05,
      "loss": 1.0761,
      "step": 2650
    },
    {
      "epoch": 0.9,
      "grad_norm": 3.4941742420196533,
      "learning_rate": 2.1003333333333333e-05,
      "loss": 1.0676,
      "step": 2700
    },
    {
      "epoch": 0.9166666666666666,
      "grad_norm": 2.775830030441284,
      "learning_rate": 2.083666666666667e-05,
      "loss": 1.0721,
      "step": 2750
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 2.2862002849578857,
      "learning_rate": 2.067e-05,
      "loss": 1.0639,
      "step": 2800
    },
    {
      "epoch": 0.95,
      "grad_norm": 2.311396837234497,
      "learning_rate": 2.0503333333333334e-05,
      "loss": 1.0656,
      "step": 2850
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 3.409688949584961,
      "learning_rate": 2.0336666666666668e-05,
      "loss": 1.0664,
      "step": 2900
    },
    {
      "epoch": 0.9833333333333333,
      "grad_norm": 2.6328890323638916,
      "learning_rate": 2.0170000000000002e-05,
      "loss": 1.0617,
      "step": 2950
    },
    {
      "epoch": 1.0,
      "grad_norm": 4.373805046081543,
      "learning_rate": 2.0003333333333336e-05,
      "loss": 1.0791,
      "step": 3000
    },
    {
      "epoch": 1.0166666666666666,
      "grad_norm": 2.668238878250122,
      "learning_rate": 1.983666666666667e-05,
      "loss": 1.0762,
      "step": 3050
    },
    {
      "epoch": 1.0333333333333334,
      "grad_norm": 2.5586130619049072,
      "learning_rate": 1.967e-05,
      "loss": 1.0834,
      "step": 3100
    },
    {
      "epoch": 1.05,
      "grad_norm": 3.658769369125366,
      "learning_rate": 1.9503333333333334e-05,
      "loss": 1.0742,
      "step": 3150
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 1.3292872905731201,
      "learning_rate": 1.9336666666666667e-05,
      "loss": 1.0715,
      "step": 3200
    },
    {
      "epoch": 1.0833333333333333,
      "grad_norm": 1.4421085119247437,
      "learning_rate": 1.917e-05,
      "loss": 1.0585,
      "step": 3250
    },
    {
      "epoch": 1.1,
      "grad_norm": 2.6529033184051514,
      "learning_rate": 1.9003333333333335e-05,
      "loss": 1.0719,
      "step": 3300
    },
    {
      "epoch": 1.1166666666666667,
      "grad_norm": 2.1529765129089355,
      "learning_rate": 1.883666666666667e-05,
      "loss": 1.0689,
      "step": 3350
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 3.778918743133545,
      "learning_rate": 1.867e-05,
      "loss": 1.0622,
      "step": 3400
    },
    {
      "epoch": 1.15,
      "grad_norm": 4.377071380615234,
      "learning_rate": 1.8503333333333333e-05,
      "loss": 1.0504,
      "step": 3450
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 2.439263105392456,
      "learning_rate": 1.8336666666666667e-05,
      "loss": 1.0613,
      "step": 3500
    },
    {
      "epoch": 1.1833333333333333,
      "grad_norm": 2.568610668182373,
      "learning_rate": 1.817e-05,
      "loss": 1.0618,
      "step": 3550
    },
    {
      "epoch": 1.2,
      "grad_norm": 2.1138463020324707,
      "learning_rate": 1.8003333333333334e-05,
      "loss": 1.0619,
      "step": 3600
    },
    {
      "epoch": 1.2166666666666668,
      "grad_norm": 2.5100619792938232,
      "learning_rate": 1.7836666666666665e-05,
      "loss": 1.0638,
      "step": 3650
    },
    {
      "epoch": 1.2333333333333334,
      "grad_norm": 2.0894153118133545,
      "learning_rate": 1.767e-05,
      "loss": 1.0718,
      "step": 3700
    },
    {
      "epoch": 1.25,
      "grad_norm": 2.617945432662964,
      "learning_rate": 1.7503333333333332e-05,
      "loss": 1.0676,
      "step": 3750
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 3.1827926635742188,
      "learning_rate": 1.7336666666666666e-05,
      "loss": 1.0795,
      "step": 3800
    },
    {
      "epoch": 1.2833333333333332,
      "grad_norm": 1.7627060413360596,
      "learning_rate": 1.717e-05,
      "loss": 1.0852,
      "step": 3850
    },
    {
      "epoch": 1.3,
      "grad_norm": 3.1185877323150635,
      "learning_rate": 1.7003333333333334e-05,
      "loss": 1.0729,
      "step": 3900
    },
    {
      "epoch": 1.3166666666666667,
      "grad_norm": 2.564716100692749,
      "learning_rate": 1.6836666666666664e-05,
      "loss": 1.0727,
      "step": 3950
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 3.745563268661499,
      "learning_rate": 1.667e-05,
      "loss": 1.0606,
      "step": 4000
    },
    {
      "epoch": 1.35,
      "grad_norm": 3.8567471504211426,
      "learning_rate": 1.6503333333333335e-05,
      "loss": 1.0772,
      "step": 4050
    },
    {
      "epoch": 1.3666666666666667,
      "grad_norm": 1.496638536453247,
      "learning_rate": 1.633666666666667e-05,
      "loss": 1.0927,
      "step": 4100
    },
    {
      "epoch": 1.3833333333333333,
      "grad_norm": 1.918047547340393,
      "learning_rate": 1.6170000000000003e-05,
      "loss": 1.0413,
      "step": 4150
    },
    {
      "epoch": 1.4,
      "grad_norm": 2.3791415691375732,
      "learning_rate": 1.6003333333333337e-05,
      "loss": 1.0513,
      "step": 4200
    },
    {
      "epoch": 1.4166666666666667,
      "grad_norm": 2.138718605041504,
      "learning_rate": 1.5836666666666667e-05,
      "loss": 1.0518,
      "step": 4250
    },
    {
      "epoch": 1.4333333333333333,
      "grad_norm": 1.5689646005630493,
      "learning_rate": 1.567e-05,
      "loss": 1.0616,
      "step": 4300
    },
    {
      "epoch": 1.45,
      "grad_norm": 2.157370090484619,
      "learning_rate": 1.5503333333333335e-05,
      "loss": 1.0674,
      "step": 4350
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 2.582406520843506,
      "learning_rate": 1.533666666666667e-05,
      "loss": 1.0554,
      "step": 4400
    },
    {
      "epoch": 1.4833333333333334,
      "grad_norm": 2.86476731300354,
      "learning_rate": 1.5170000000000002e-05,
      "loss": 1.054,
      "step": 4450
    },
    {
      "epoch": 1.5,
      "grad_norm": 4.222433090209961,
      "learning_rate": 1.5003333333333333e-05,
      "loss": 1.0682,
      "step": 4500
    },
    {
      "epoch": 1.5166666666666666,
      "grad_norm": 2.65368914604187,
      "learning_rate": 1.4836666666666668e-05,
      "loss": 1.058,
      "step": 4550
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 2.522167444229126,
      "learning_rate": 1.467e-05,
      "loss": 1.0681,
      "step": 4600
    },
    {
      "epoch": 1.55,
      "grad_norm": 2.5478501319885254,
      "learning_rate": 1.4503333333333334e-05,
      "loss": 1.0666,
      "step": 4650
    },
    {
      "epoch": 1.5666666666666667,
      "grad_norm": 2.143399953842163,
      "learning_rate": 1.4336666666666666e-05,
      "loss": 1.0486,
      "step": 4700
    },
    {
      "epoch": 1.5833333333333335,
      "grad_norm": 1.3700613975524902,
      "learning_rate": 1.417e-05,
      "loss": 1.0527,
      "step": 4750
    },
    {
      "epoch": 1.6,
      "grad_norm": 1.989356279373169,
      "learning_rate": 1.4003333333333334e-05,
      "loss": 1.0625,
      "step": 4800
    },
    {
      "epoch": 1.6166666666666667,
      "grad_norm": 2.582811117172241,
      "learning_rate": 1.3836666666666666e-05,
      "loss": 1.0642,
      "step": 4850
    },
    {
      "epoch": 1.6333333333333333,
      "grad_norm": 3.9493069648742676,
      "learning_rate": 1.367e-05,
      "loss": 1.0637,
      "step": 4900
    },
    {
      "epoch": 1.65,
      "grad_norm": 1.2932828664779663,
      "learning_rate": 1.3503333333333333e-05,
      "loss": 1.0635,
      "step": 4950
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 2.2934272289276123,
      "learning_rate": 1.3336666666666667e-05,
      "loss": 1.0622,
      "step": 5000
    },
    {
      "epoch": 1.6833333333333333,
      "grad_norm": 2.9885525703430176,
      "learning_rate": 1.3170000000000001e-05,
      "loss": 1.06,
      "step": 5050
    },
    {
      "epoch": 1.7,
      "grad_norm": 2.3171870708465576,
      "learning_rate": 1.3003333333333335e-05,
      "loss": 1.0549,
      "step": 5100
    },
    {
      "epoch": 1.7166666666666668,
      "grad_norm": 1.5297859907150269,
      "learning_rate": 1.2836666666666667e-05,
      "loss": 1.0834,
      "step": 5150
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 3.4478909969329834,
      "learning_rate": 1.267e-05,
      "loss": 1.0568,
      "step": 5200
    },
    {
      "epoch": 1.75,
      "grad_norm": 3.8220841884613037,
      "learning_rate": 1.2503333333333334e-05,
      "loss": 1.0559,
      "step": 5250
    },
    {
      "epoch": 1.7666666666666666,
      "grad_norm": 1.3755089044570923,
      "learning_rate": 1.2336666666666667e-05,
      "loss": 1.0647,
      "step": 5300
    },
    {
      "epoch": 1.7833333333333332,
      "grad_norm": 3.6182947158813477,
      "learning_rate": 1.217e-05,
      "loss": 1.0834,
      "step": 5350
    },
    {
      "epoch": 1.8,
      "grad_norm": 1.4674428701400757,
      "learning_rate": 1.2003333333333332e-05,
      "loss": 1.0734,
      "step": 5400
    },
    {
      "epoch": 1.8166666666666667,
      "grad_norm": 2.41585636138916,
      "learning_rate": 1.1836666666666666e-05,
      "loss": 1.0692,
      "step": 5450
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 3.3814659118652344,
      "learning_rate": 1.167e-05,
      "loss": 1.0614,
      "step": 5500
    },
    {
      "epoch": 1.85,
      "grad_norm": 3.802006721496582,
      "learning_rate": 1.1503333333333332e-05,
      "loss": 1.0631,
      "step": 5550
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 2.4842872619628906,
      "learning_rate": 1.1336666666666668e-05,
      "loss": 1.0605,
      "step": 5600
    },
    {
      "epoch": 1.8833333333333333,
      "grad_norm": 3.538924217224121,
      "learning_rate": 1.1170000000000001e-05,
      "loss": 1.0577,
      "step": 5650
    },
    {
      "epoch": 1.9,
      "grad_norm": 1.7570469379425049,
      "learning_rate": 1.1003333333333334e-05,
      "loss": 1.0552,
      "step": 5700
    },
    {
      "epoch": 1.9166666666666665,
      "grad_norm": 2.376011371612549,
      "learning_rate": 1.0836666666666667e-05,
      "loss": 1.0637,
      "step": 5750
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 1.3394322395324707,
      "learning_rate": 1.0670000000000001e-05,
      "loss": 1.0704,
      "step": 5800
    },
    {
      "epoch": 1.95,
      "grad_norm": 3.295848846435547,
      "learning_rate": 1.0503333333333333e-05,
      "loss": 1.0839,
      "step": 5850
    },
    {
      "epoch": 1.9666666666666668,
      "grad_norm": 2.5469655990600586,
      "learning_rate": 1.0336666666666667e-05,
      "loss": 1.0703,
      "step": 5900
    },
    {
      "epoch": 1.9833333333333334,
      "grad_norm": 2.265617847442627,
      "learning_rate": 1.0170000000000001e-05,
      "loss": 1.0775,
      "step": 5950
    },
    {
      "epoch": 2.0,
      "grad_norm": 3.4354069232940674,
      "learning_rate": 1.0003333333333333e-05,
      "loss": 1.0683,
      "step": 6000
    },
    {
      "epoch": 2.0166666666666666,
      "grad_norm": 2.3813393115997314,
      "learning_rate": 9.836666666666667e-06,
      "loss": 1.0613,
      "step": 6050
    },
    {
      "epoch": 2.033333333333333,
      "grad_norm": 1.649203896522522,
      "learning_rate": 9.67e-06,
      "loss": 1.0592,
      "step": 6100
    },
    {
      "epoch": 2.05,
      "grad_norm": 1.9629374742507935,
      "learning_rate": 9.503333333333333e-06,
      "loss": 1.0639,
      "step": 6150
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 1.7279549837112427,
      "learning_rate": 9.336666666666666e-06,
      "loss": 1.0757,
      "step": 6200
    },
    {
      "epoch": 2.0833333333333335,
      "grad_norm": 3.2618818283081055,
      "learning_rate": 9.17e-06,
      "loss": 1.0717,
      "step": 6250
    },
    {
      "epoch": 2.1,
      "grad_norm": 3.274251937866211,
      "learning_rate": 9.003333333333334e-06,
      "loss": 1.062,
      "step": 6300
    },
    {
      "epoch": 2.1166666666666667,
      "grad_norm": 3.1454195976257324,
      "learning_rate": 8.836666666666668e-06,
      "loss": 1.0671,
      "step": 6350
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 1.1588704586029053,
      "learning_rate": 8.67e-06,
      "loss": 1.0608,
      "step": 6400
    },
    {
      "epoch": 2.15,
      "grad_norm": 4.711724758148193,
      "learning_rate": 8.503333333333334e-06,
      "loss": 1.0606,
      "step": 6450
    },
    {
      "epoch": 2.1666666666666665,
      "grad_norm": 2.7904813289642334,
      "learning_rate": 8.336666666666668e-06,
      "loss": 1.0635,
      "step": 6500
    },
    {
      "epoch": 2.183333333333333,
      "grad_norm": 1.9387072324752808,
      "learning_rate": 8.17e-06,
      "loss": 1.0712,
      "step": 6550
    },
    {
      "epoch": 2.2,
      "grad_norm": 2.804077386856079,
      "learning_rate": 8.003333333333334e-06,
      "loss": 1.0586,
      "step": 6600
    },
    {
      "epoch": 2.216666666666667,
      "grad_norm": 1.5307345390319824,
      "learning_rate": 7.836666666666667e-06,
      "loss": 1.0763,
      "step": 6650
    },
    {
      "epoch": 2.2333333333333334,
      "grad_norm": 2.288374662399292,
      "learning_rate": 7.67e-06,
      "loss": 1.0599,
      "step": 6700
    },
    {
      "epoch": 2.25,
      "grad_norm": 3.1762654781341553,
      "learning_rate": 7.503333333333334e-06,
      "loss": 1.0555,
      "step": 6750
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 2.639029026031494,
      "learning_rate": 7.336666666666667e-06,
      "loss": 1.0555,
      "step": 6800
    },
    {
      "epoch": 2.283333333333333,
      "grad_norm": 3.1166841983795166,
      "learning_rate": 7.17e-06,
      "loss": 1.0461,
      "step": 6850
    },
    {
      "epoch": 2.3,
      "grad_norm": 2.6473348140716553,
      "learning_rate": 7.003333333333334e-06,
      "loss": 1.0761,
      "step": 6900
    },
    {
      "epoch": 2.3166666666666664,
      "grad_norm": 1.5416895151138306,
      "learning_rate": 6.836666666666667e-06,
      "loss": 1.0584,
      "step": 6950
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 4.031094551086426,
      "learning_rate": 6.67e-06,
      "loss": 1.0873,
      "step": 7000
    }
  ],
  "logging_steps": 50,
  "max_steps": 9000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 17394599967888.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
